{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressione lineare multipla\n",
    "La regressione lineare multipla o multivariata √® un caso di regressione lineare con due o pi√π variabili indipendenti.\n",
    "\n",
    "Se ci sono solo due variabili indipendenti, la funzione di regressione stimata √® ùëì(ùë•‚ÇÅ, ùë•‚ÇÇ) = ùëè‚ÇÄ + ùëè‚ÇÅùë•‚ÇÅ + ùëè‚ÇÇùë•‚ÇÇ. Rappresenta un piano di regressione in uno spazio tridimensionale. L'obiettivo della regressione √® determinare i valori dei pesi ùëè‚ÇÄ, ùëè‚ÇÅ e ùëè‚ÇÇ in modo tale che questo piano sia il pi√π vicino possibile alle risposte effettive, producendo al contempo l'SSR minimo.\n",
    "\n",
    "Il caso di pi√π di due variabili indipendenti √® simile, ma pi√π generale. La funzione di regressione stimata √® ùëì(ùë•‚ÇÅ, ‚Ä¶, ùë•·µ£) = ùëè‚ÇÄ + ùëè‚ÇÅùë•‚ÇÅ + ‚ãØ +ùëè·µ£ùë•·µ£, e ci sono ùëü + 1 pesi da determinare quando il numero di input √® ùëü."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementiamo la Regressione lineare multipla con scikit-learn\n",
    "√à possibile implementare una regressione lineare multipla seguendo gli stessi passaggi della regressione semplice. La differenza principale √® che ora il nostro x array avr√† due o pi√π colonne.\n",
    "\n",
    "#### Passaggi 1 e 2: importiamo pacchetti e classi e forniamo i dati\n",
    "\n",
    "Innanzitutto, importiamo numpy e sklearn.linear_model.LinearRegression e forniamo input e output noti:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "x = [\n",
    "  [0, 1], [5, 1], [15, 2], [25, 5], [35, 11], [45, 15], [55, 34], [60, 35]\n",
    "]\n",
    "y = [4, 5, 20, 14, 32, 22, 38, 43]\n",
    "x, y = np.array(x), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1],\n",
       "       [ 5,  1],\n",
       "       [15,  2],\n",
       "       [25,  5],\n",
       "       [35, 11],\n",
       "       [45, 15],\n",
       "       [55, 34],\n",
       "       [60, 35]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4,  5, 20, 14, 32, 22, 38, 43])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come abbiamo gi√† detto, nella regressione lineare multipla, x √® una matrice bidimensionale con almeno due colonne, mentre y di solito √® una matrice unidimensionale. Questo √® un semplice esempio di regressione lineare multipla e x ha esattamente due colonne.\n",
    "\n",
    "### Passaggio 3: creiamo un modello e lo adattiamo\n",
    "\n",
    "Il passaggio successivo consiste nel creare il modello di regressione come istanza LinearRegressione per poi allenarlo con .fit():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression().fit(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il risultato di questa istruzione √® la variabile model che fa riferimento all'oggetto di tipo LinearRegression. Rappresenta il modello di regressione adattato ai dati esistenti.\n",
    "\n",
    "### Passaggio 4: ottieniamo i risultati\n",
    "\n",
    "√à possibile ottenere le propriet√† del modello come nel caso della regressione lineare semplice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coefficient of determination: 0.8615939258756776\n",
      "intercept: 5.52257927519819\n",
      "coefficients: [0.44706965 0.25502548]\n"
     ]
    }
   ],
   "source": [
    "r_sq = model.score(x, y)\n",
    "print(f\"coefficient of determination: {r_sq}\")\n",
    "\n",
    "\n",
    "print(f\"intercept: {model.intercept_}\")\n",
    "\n",
    "\n",
    "print(f\"coefficients: {model.coef_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si ottiene il valore di ùëÖ¬≤ utilizzando .score() i valori degli stimatori dei coefficienti di regressione con .intercept_e .coef_. Ancora una volta, .intercept_ mantiene il bias ùëè‚ÇÄ, mentre ora .coef_√® un array contenente ùëè‚ÇÅ e ùëè‚ÇÇ.\n",
    "\n",
    "In questo esempio, l'intercetta √® circa 5,52 e questo √® il valore della risposta prevista quando ùë•‚ÇÅ = ùë•‚ÇÇ = 0. Un aumento di ùë•‚ÇÅ di 1 produce un aumento della risposta prevista di 0,45. Allo stesso modo, quando ùë•‚ÇÇ cresce di 1, la risposta aumenta di 0,26.\n",
    "\n",
    "### Passaggio 5: prevedere la risposta\n",
    "\n",
    "Anche le previsioni funzionano allo stesso modo del caso della regressione lineare semplice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted response:\n",
      "[ 5.77760476  8.012953   12.73867497 17.9744479  23.97529728 29.4660957\n",
      " 38.78227633 41.27265006]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x)\n",
    "print(f\"predicted response:\\n{y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La risposta prevista si ottiene con .predict(), che equivale alla seguente formula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted response:\n",
      "[ 5.77760476  8.012953   12.73867497 17.9744479  23.97529728 29.4660957\n",
      " 38.78227633 41.27265006]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.intercept_ + np.sum(model.coef_ * x, axis=1)\n",
    "print(f\"predicted response:\\n{y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "√à possibile prevedere i valori di output moltiplicando ciascuna colonna dell'input con il peso appropriato, per poi sommare questo valore con l'intercetta.\n",
    "\n",
    "Possiamo applicare questo modello anche a nuovi dati:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.77760476,  7.18179502,  8.58598528,  9.99017554, 11.3943658 ])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_new = np.arange(10).reshape((-1, 2))\n",
    "x_new\n",
    "\n",
    "y_new = model.predict(x_new)\n",
    "y_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primo Esempio pratico\n",
    "### Analizzando il dataset cardata.csv possiamo prevedere le emissioni CO2 in base alle dimensioni del motore e al peso dell'auto, con la possibilit√† di analizzare 2 carretteristiche invece che una sola possiamo infatti rendere la previsione pi√π accurata.\n",
    "\n",
    "\n",
    "Prima cosa da fare importiamo scikit-learn per il modello e pandas per leggere il csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "from sklearn import linear_model\n",
    "df = pandas.read_csv(\"cardata.csv\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analizzato il dataset possiamo creare le nostre variabili:\n",
    "- X per contenente i valori indipendenti;\n",
    "- y per contenere i valori target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poi utilizziamo il nostro modello e il metodo .fit() per allenarerlo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['Weight', 'Volume']]\n",
    "y = df['CO2']\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_sq = regr.score(X, y)\n",
    "print(f\"coefficient of determination: {r_sq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regr.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import PredictionErrorDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "display = PredictionErrorDisplay(y_true=y, y_pred=y_pred)\n",
    "display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora possiamo prevedere i valori di CO2 in base al peso e al volume di un'auto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict the CO2 emission of a car where the weight is 2300kg, and the volume is 1300cm3:\n",
    "predictedCO2 = regr.predict([[2300, 1300]])\n",
    "print(predictedCO2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passiamo al Coefficiente\n",
    "Abbiamo detto che il coefficiente √® un fattore che descrive la relazione con una variabile sconosciuta.\n",
    "\n",
    "In questo caso, possiamo chiedere il valore del coefficiente di peso rispetto a CO2 e di volume rispetto a CO2. Le risposte che otteniamo ci dicono cosa accadrebbe se aumentassimo o diminuissimo uno dei valori indipendenti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(regr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La matrice dei risultati rappresenta i valori dei coefficienti di peso e volume.\n",
    "\n",
    "Peso: 0,00755095\n",
    "Volume: 0,00780526\n",
    "\n",
    "Questi valori ci dicono che se il peso aumenta di 1kg, l'emissione di CO2 aumenta di 0,00755095g.\n",
    "\n",
    "E se la cilindrata (Volume) aumenta di 1 centimentro cubo, l'emissione di CO2 aumenta di 0,00780526 g.\n",
    "\n",
    "Considerando che sia una supposizione giusta andiamo a fare altre prove.\n",
    "\n",
    "Abbiamo gi√† previsto che se un'auto con motore da 1300 cm 3 pesa 2300 kg, l'emissione di CO2 sar√† di circa 107 g.\n",
    "\n",
    "Cosa succede se aumentiamo il peso di 1000 kg?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictedCO2 = regr.predict([[3300, 1300]])\n",
    "\n",
    "print(predictedCO2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abbiamo previsto che un'auto con motore da 1,3 litri e peso di 3300 kg rilascer√† circa 115 grammi di CO2 per ogni chilometro percorso.\n",
    "\n",
    "Ci√≤ dimostra che il coefficiente di 0,00755095 √® corretto:\n",
    "\n",
    "107,2087328 + (1000 * 0,00755095) = 114,75968"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_stand.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_stand.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_scaler = StandardScaler()\n",
    "\n",
    "X_stand = X_scaler.fit_transform(X)\n",
    "\n",
    "X_stand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primo Esercizio\n",
    " Utilizzate la linear regression multipla per analizzare il dataframe a questo link https://www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression utilizzate i dati sulle ore di studio e le ore di sonno , allenate l'algoritmo, testatelo e poi realizzate i vari grafici"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Secono Esercizio\n",
    "Utilizzate la linear regression multipla per analizzare il dataframe a questo link https://www.kaggle.com/datasets/quantbruce/real-estate-price-prediction utilizzate i dati sulle et√† delle case e la distanza dalla metro per prevedere il costo della casa , allenate l'algoritmo, testatelo e poi realizzate i vari grafici"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAVE THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diz = {\"chiave\":\"valore\",\"chiave2\":\"valore2\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"file.pkl\", \"wb\") as f:\n",
    "    pickle.dump(diz, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"file.pkl\", \"rb\") as f:\n",
    "    diz2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINED_MODEL = \"trained_model.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TRAINED_MODEL, \"wb\") as f:\n",
    "    pickle.dump(regr, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL DEPLOYMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TRAINED_MODEL, \"rb\") as f:\n",
    "    trained_regressor = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressione polinomiale\n",
    "Se i nostri punti chiaramente non si adattano a una regressione lineare (una linea retta che passa attraverso tutti i punti dati), potrebbe essere ideale utilizzare la regressione polinomiale.\n",
    "\n",
    "La regressione polinomiale, come la regressione lineare, utilizza la relazione tra le variabili X e y per trovare il modo migliore per tracciare una linea attraverso i punti dati.\n",
    "\n",
    "\n",
    "\n",
    "![grafico](./dati/img/img_polynomial_regression.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regressione polinomiale con scikit-learn\n",
    "L'implementazione della regressione polinomiale con scikit-learn √® molto simile alla regressione lineare. Ma c'√® solo un passaggio in pi√π: dobbiamo trasformare l'array di input per includere termini non lineari come ùë•¬≤.\n",
    "\n",
    "### Passaggio 1: importiamo pacchetti e classi\n",
    "\n",
    "Oltre a numpy e sklearn.linear_model.LinearRegression, dobbiamo anche importare la classe PolynomialFeatures da sklearn.preprocessing:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'importazione √® ora completata e abbiamo tutto ci√≤ di cui abbiamo bisogno per lavorare.\n",
    "\n",
    "### Passaggio 2a: forniamo i dati\n",
    "\n",
    "Questo passaggio definisce l'input e l'output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([5, 15, 25, 35, 45, 55]).reshape((-1, 1))\n",
    "y = np.array([15, 11, 2, 8, 25, 32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora abbiamo l'input e l'output in un formato adatto. Dobbiamo ricordarci che √® necessario che l'input sia un array bidimensionale . Ecco perch√© .reshape() viene utilizzato.\n",
    "\n",
    "### Passaggio 2b: trasformarmiamo i dati di input\n",
    "\n",
    "Questo √® il nuovo passaggio che dobbiamo implementare per la regressione polinomiale!\n",
    "\n",
    "Come abbiamo imparato in precedenza, dobbiamo includere ùë•¬≤ come funzionalit√† aggiuntive quando implementiamo la regressione polinomiale. Per questo motivo, dobbiamo trasformare l'array di input X per contenere eventuali colonne aggiuntive con i valori di ùë•¬≤ ed eventualmente pi√π funzionalit√†.\n",
    "\n",
    "√à possibile trasformare l'array di input in diversi modi, ad esempio utilizzando insert() from numpy. Ma nel nostro caso la funzione PolynomialFeatures √® molto pi√π conveniente per questo scopo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = PolynomialFeatures(degree=2, include_bias=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La variabile transformer si riferisce a un'istanza PolynomialFeatures che √® possibile utilizzare per trasformare l'input X.\n",
    "\n",
    "√à possibile fornire diversi parametri facoltativi per PolynomialFeatures:\n",
    "\n",
    "- degree √® un numero intero ( 2 per impostazione predefinita) che rappresenta il grado della funzione di regressione polinomiale.\n",
    "- interaction_only √® un valore booleano ( False per impostazione predefinita) che decide se includere solo le funzionalit√† di interazione ( True) o tutte le funzionalit√† ( False).\n",
    "- include_bias √® un valore booleano ( True per impostazione predefinita) che decide se includere la colonna di valori ( True) o meno ( False).\n",
    "\n",
    "In questo esempio vengono utilizzati i valori predefiniti di tutti i parametri tranne include_bias. A volte vorrai sperimentare il grado della funzione e pu√≤ essere utile per la leggibilit√† fornire comunque questo argomento.\n",
    "\n",
    "Prima di applicare transformer √® necessario munirlo di .fit(), una volta fatto √® pronto per creare un nuovo array di input modificato. Utilizziamo .transform() per farlo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.fit(x)\n",
    "x_ = transformer.transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questa trasformazione dell'array di input con .transform() prende l'array di input come argomento e restituisce l'array modificato.\n",
    "\n",
    "Per velocit√† si pu√≤ anche usare .fit_transform() per sostituire le tre affermazioni precedenti con una sola:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ = PolynomialFeatures(degree=2, include_bias=False).fit_transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con .fit_transform(), stiamo adattando e trasformando l'array di input in un'unica istruzione. Anche questo metodo accetta l'array di input e fa effettivamente la stessa cosa di .fit() e .transform()c hiamato in quest'ordine. Restituisce anche l'array modificato. Ecco come appare il nuovo array di input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'array di input modificato contiene due colonne: una con gli input originali e l'altra con i relativi quadrati.\n",
    "\n",
    "### Passaggio 3: creiamo un modello e adattalo\n",
    "\n",
    "Anche questo passaggio √® lo stesso del caso della regressione lineare. Creiamo e adattiamo il ‚Äã‚Äãmodello:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression().fit(x_, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il modello di regressione √® ora creato e adattato. √à pronto per l'applicazione. Ricordiamoci che il primo argomento di .fit() √® l' array di input modificato x_ e non l'originale x.\n",
    "\n",
    "### Passaggio 4: ottieniamo i risultati\n",
    "\n",
    "√à possibile ottenere le propriet√† del modello allo stesso modo del caso della regressione lineare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_sq = model.score(x_, y)\n",
    "print(f\"coefficient of determination: {r_sq}\")\n",
    "\n",
    "\n",
    "print(f\"intercept: {model.intercept_}\")\n",
    "\n",
    "\n",
    "print(f\"coefficients: {model.coef_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ancora una volta, .score() ritorna ùëÖ¬≤. Anche il suo primo argomento √® l'input modificato x_, non x. I valori dei pesi sono associati a .intercept_ e .coef_. Qui .intercept_ rappresenta ùëè‚ÇÄ, mentre .coef_ fa riferimento all'array che contiene ùëè‚ÇÅ e ùëè‚ÇÇ.\n",
    "\n",
    "### Passaggio 5: prevedere la risposta\n",
    "\n",
    "Se vogliamo ottenere la risposta prevista, usiamo semplicemente .predict(), ma ricordiamo che l'argomento dovrebbe essere l'input modificato x_ anzich√© il vecchio x:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_)\n",
    "print(f\"predicted response:\\n{y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come possiamo vedere, la previsione funziona quasi allo stesso modo del caso della regressione lineare. Richiede solo l'input modificato anzich√© l'originale.\n",
    "\n",
    "√à possibile applicare una procedura identica se si hanno pi√π variabili di input . Avremo un array di input con pi√π di una colonna, ma tutto il resto sar√† lo stesso. Ecco un esempio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import packages and classes\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Step 2 a: Provide data\n",
    "x = [\n",
    "  [0, 1], [5, 1], [15, 2], [25, 5], [35, 11], [45, 15], [55, 34], [60, 35]\n",
    "]\n",
    "y = [4, 5, 20, 14, 32, 22, 38, 43]\n",
    "x, y = np.array(x), np.array(y)\n",
    "\n",
    "# Step 2 b: Transform input data\n",
    "x_ = PolynomialFeatures(degree=2, include_bias=False).fit_transform(x)\n",
    "\n",
    "# Step 3: Create a model and fit it\n",
    "model = LinearRegression().fit(x_, y)\n",
    "\n",
    "# Step 4: Get results\n",
    "r_sq = model.score(x_, y)\n",
    "intercept, coefficients = model.intercept_, model.coef_\n",
    "\n",
    "# Step 5: Predict response\n",
    "y_pred = model.predict(x_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questo esempio di regressione produce i seguenti risultati e previsioni:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"coefficient of determination: {r_sq}\")\n",
    "\n",
    "\n",
    "print(f\"intercept: {intercept}\")\n",
    "\n",
    "\n",
    "print(f\"coefficients:\\n{coefficients}\")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"predicted response:\\n{y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possiamo notare che la regressione polinomiale ha prodotto un coefficiente di determinazione pi√π elevato rispetto alla regressione lineare multipla per lo stesso problema. Inizialmente si potrebbe pensare che ottenere un ùëÖ¬≤ cos√¨ grande sia un ottimo risultato.\n",
    "\n",
    "Tuttavia, nelle situazioni del mondo reale, anche avere un modello complesso e ùëÖ¬≤ molto vicino a uno potrebbe essere un problema di overfitting (di cui parleremo a brevissimo). Per verificare le prestazioni di un modello, dovremmo testarlo con nuovi dati, ovvero con osservazioni non utilizzate per adattare o addestrare il modello."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Altro esempio\n",
    "\n",
    "Nell'esempio seguente abbiamo immortalato 18 automobili mentre transitavano davanti ad un determinato casello.\n",
    "\n",
    "Abbiamo registrato la velocit√† dell'auto e l'ora del giorno in cui √® avvenuto il passaggio.\n",
    "\n",
    "L'asse x rappresenta le ore del giorno e l'asse y rappresenta la velocit√†, iniziamo disegnando un grafico a dispersione:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = [1,2,3,5,6,7,8,9,10,12,13,14,15,16,18,19,21,22]\n",
    "y = [100,90,80,60,60,55,60,65,70,70,75,76,78,79,90,99,99,100]\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Usiamo numpy per creare un modello polinomiale:\n",
    "import numpy\n",
    "mymodel = numpy.poly1d(numpy.polyfit(x, y, 3))\n",
    "\n",
    "#Quindi specifichiamo come verr√† visualizzata la linea, iniziamo dalla posizione 1 e terminiamo alla posizione 22:\n",
    "\n",
    "myline = numpy.linspace(1, 22, 100)\n",
    "\n",
    "#Disegnamo il grafico a dispersione originale:\n",
    "\n",
    "plt.scatter(x, y)\n",
    "\n",
    "#Disegna la linea di regressione polinomiale:\n",
    "\n",
    "plt.plot(myline, mymodel(myline))\n",
    "\n",
    "#Visualizza il diagramma:\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturalmente √® importante sapere quanto √® buona la relazione tra i valori degli assi x e y, se non esiste alcuna relazione la regressione polinomiale non pu√≤ essere utilizzata per prevedere nulla.\n",
    "\n",
    "Misuriamo la relazione con il valore r-quadrato che ricordiamo varia da 0 a 1, dove 0 significa nessuna relazione e 1 significa correlato al 100%.\n",
    "\n",
    "Come abbiamo gi√† visto, Python e il modulo Sklearn calcoleranno questo valore per noi, tutto ci√≤ che dobbiamo fare √® alimentarlo con gli array X e y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "x = [1,2,3,5,6,7,8,9,10,12,13,14,15,16,18,19,21,22]\n",
    "y = [100,90,80,60,60,55,60,65,70,70,75,76,78,79,90,99,99,100]\n",
    "\n",
    "mymodel = numpy.poly1d(numpy.polyfit(x, y, 3))\n",
    "\n",
    "print(r2_score(y, mymodel(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Il risultato 0,94 mostra che esiste un'ottima relazione e possiamo utilizzare la regressione polinomiale nelle previsioni future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ora torniamo alle slide per parlare dell'overfitting e underfitting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
